---
title: "st538 Project 2 Week 7"
author: "Philip Ourso"
date: "5/14/2022"
output:
  word_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```
  
```{r, echo=F, warning=F, message=F}
library(tidyverse)

library(rpart)
library(tree)
library(randomForest)
library(ipred)
library(gbm)
library(xgboost)

```
  
```{r, echo=F, message=F, warning=F, include=F}
# copied from: Week2_LR2_code.R
if(! (packageVersion("ggplot2") >= "2.0.0")) {
  stop("This version of stat_qqline require ggplot2 version 2.0.0 or greater")
}

StatQQLine <- ggproto("StatQQLine", Stat,
                      compute_group = function(data, scales, distribution = qnorm, dparams = list()) {
                        data <- remove_missing(data, na.rm = TRUE, "sample", name = "stat_qqline")
                        y <- quantile(data$sample, c(0.25, 0.75))
                        x <- do.call(distribution, c(list(p = c(0.25, 0.75)), dparams))
                        slope <- diff(y)/diff(x)
                        int <- y[1L] - slope * x[1L]
                        data.frame(slope = slope, intercept = int)
                      },
                      
                      required_aes = c("sample")
)

stat_qqline <- function(mapping = NULL, data = NULL, geom = "abline",
                        position = "identity", na.rm = FALSE, show.legend = NA,
                        distribution = qnorm, dparams = list(),
                        inherit.aes = TRUE, ...) {
  layer(
    stat = StatQQLine, data = data, mapping = mapping, geom = geom,
    position = position, show.legend = show.legend, inherit.aes = inherit.aes,
    params = list(na.rm = na.rm, distribution = distribution, dparams = dparams, ...)
  )
}
```     
  
     
## st538 Project \#2, Group \#4 - Interim Report, week 7  
   
### Introduction  
  
    For this project we chose dataset 5, the Kaggle Product Line Performance Challenge. Bosch's challenge to Kaggle users is to predict component failure on the basis of measurements made throughout the manufacturing process. Each component, or observation, includes a large amount of numerical, categorical and date data. These features are anonymized, but identified by a feature number, as well as the line and station ID on which the measurement was made. The date features indicate the date on which the measurement was made.  
  
    The components are uniquely identified and labelled as to whether they failed quality control. Given that a successful manufacturing company would naturally minimize failures, it is expected that the data would be highly imbalanced.  
  
```{r, echo=F, warning=F, message=F, include=F, eval=F}  
#C:/Users/pourso/Google Drive/School/st538/st538_code

# read data
set.seed(1)
n = 200000
sel = sample(c(0,1), nrow(df_c), replace = T, prob = c(1/4,3/4))

# can't load all categorical, so start w/ first 200k
df_c = read.csv("./bosch-production-line-performance/train_categorical.csv", header = T, nrows = n)
dim(df_c)
write.csv(df_c[sel==1,], 'train_cat.csv')
write.csv(df_c[sel==0,], 'test_cat.csv')
rm(df_c)
gc()


#df_n = read.csv(unz("./bosch-production-line-performance/train_numeric.csv.zip",
#                  "train_numeric.csv"
#                  ), header = T)
df_n = read.csv("./bosch-production-line-performance/train_numeric.csv", header = T, nrows = n)
dim(df_n)
write.csv(df_n[sel==1,], 'train_num.csv')
write.csv(df_n[sel==0,], 'test_num.csv')
rm(df_n)
gc()

df_d = read.csv("./bosch-production-line-performance/train_date.csv", header = T, nrows = n)
dim(df_d)
write.csv(df_d[sel==1,], 'train_dat.csv')
write.csv(df_d[sel==0,], 'test_dat.csv')
rm(df_d)
gc()

# split train, val w/ lower portion of training data
train_n = 50000
df_n_50k = read.csv("./train_num.csv", header = T, nrows = train_n)
df_c_50k = read.csv("./train_cat.csv", header = T, nrows = train_n)
df_d_50k = read.csv("./train_dat.csv", header = T, nrows = train_n)

train_sel = sample(c(0,1), nrow(df_n_50k), replace = T, prob = c(1/5,4/5))

# 40k for training
df_nc_train = merge(df_c_50k[train_sel==1,], df_n_50k[train_sel==1,])

# 10k for val
df_nc_val = merge(df_c_50k[train_sel==0,], df_n_50k[train_sel==0,])

write.csv(df_nc_train, 'train_40k.csv')
write.csv(df_nc_val, 'val_10k.csv')

rm(df_c_50k)
rm(df_n_50k)
rm(df_d_50k)
gc()
# NAs?
```    
  
```{r, echo=F, message=F, warning=F}  
# load data
df_nc_train = read.csv('train_40k.csv', header = T)

#df_nc_val = read.csv('val_10k.csv', header = T)

# separate features, response, ignore ID and X
df_nc_train_y = df_nc_train[,ncol(df_nc_train)]
df_nc_train_X = df_nc_train[,4:(ncol(df_nc_train)-1)]

mean(df_nc_train_y) # only 0.55% faulty

df_t = data.frame(x=df_nc_train_X, y=factor(df_nc_train_y))

# explicitly convert cat data to factors
df_t_tmp = df_t
df_t_tmp[, 1:2140] <- lapply(df_t_tmp[, 1:2140], as.factor)

# FIXME: this will vary from sample to sample
# remove cols of all NAs
tmp = df_t_tmp[, colSums(is.na(df_t_tmp)) < nrow(df_t_tmp)]
```    
  
### Feature extraction and description    
  
    As the specifying the exact features would likely reveal trade secrets, features were anonymized and represented in the format:     
> L<Line #>_S<Station #>_F<Feature #>  
  
  
    The precise meaning of "line", "station" and "feature" is not provided, but given the manufacturing context it is likely that features refer to specfic measurements, stations refer to specific tools and lines refer to the overall sequence of operations. It would not be surprising for there to be redundancy in the lines and stations: each line might produce the same component and hence consist of the same types of stations. Furthermore, each line would likely have redundant stations to enable increased throughput.      
     
    Assuming redundancies in lines and stations, then the feature labels themselves _could_ serve as features themselves: which specific station processed a component is useful information, as the individual station performance would vary.           
     
    The datasets were provided in several CSV files: training and test data were separated and presumably no response data was provided for test data. Training data was exclusively used for this project and was split across three files: numerical, categorical and datetime data. The datetime data indicated when the numerical and categorical data was collected for each observation.    
  
    There were nearly 1,200,000 observations, with 968 numerical features, 2140 categorical features and their associated datetime data.    
       
### Building classifiers and results     
   
    Given the sheer volume of data, it was necessary to reduce the dataset to something that could be tackled on a personal computer. Using the full dataset would require access to massively parallel cloud computing resources. While the entire numerical data set could be loaded into memory, it was impossible to do so for the categorical data, so only 200,000 rows was read from the numerical and categorical CSVs. Datetime data was not used.     
  
    Once in memory, a train-test split of the 200,000 observations in a ratio of 3:1 was written back to disk, with categorical and numerical data joined. Even 150,000 observations was infeasible, so a subset of 50,000 observations was loaded and split 4:1 to serve as training and validation data.      
   
    Features and response were separated; defective components accounted for ~0.55% of the training data, confirming the highly imbalanced nature of the dataset. A classification regression tree was fit to the full feature set and 6 variables were considered for the first split. Of the 6 features, L1_S24_F1846 was found to be the most important and a split of:  
> L1_S24_F1846 < -0.31  
  
  
    was used to to assign observations to the defective component class. This was found to improve upon the null model of just guessing the majority class (non-defective), but over 37,000 observations were missing data for this feature, so it was not a very relevant split.  
    
    The misclassification rate was ~0.547% with the below table indicating a non-trival amount of non-defective parts were predicted to be defective. Given a better model, this might be an acceptable trade-off: while it is expensive to fail good parts, it is likely more expensive to sell bad parts, considering both the cost of the customer return and the impact to brand image.  
  
          | good  | bad  
    --------------------  
    pgood | 39786 | 5
     pbad |   214 | 8    
   
```{r, echo=F, message=F, warning=F, include=F}
## single classification decision tree

# classification tree
rpart_tmp = rpart(y~., data = tmp, method = 'class')

# display the results
printcp(rpart_tmp) 

# visualize cross-validation results
plotcp(rpart_tmp) 

# detailed summary of splits
summary(rpart_tmp, pretty = T) 

# plot tree
plot(rpart_tmp, uniform=TRUE,
     main="Classification Tree")
text(rpart_tmp, use.n=TRUE, all=TRUE, cex=.5)

# Fitted Values in class probabilities
pred_tmp = predict(rpart_tmp, tmp[,1:(ncol(tmp)-1)])

# Fitted response labels
res_tmp = ifelse(pred_tmp[, 1] > 0.5, 0, 1)

# Plotting the data with fitted labels based on the model
#plot(sim.data1$x.1, sim.data1$x.2, pch = as.numeric(sim.data1$y),
#     col = factor(res))

# Misclassification errors in the training data
table(tmp$y, res_tmp)

```    
  
```{r, echo=F, message=F, warning=F, include=F, eval=F}
## single classification decision tree

# classification tree
rpart_t = rpart(y~., data = df_t, method = 'class')

# display the results
printcp(rpart_t) 

# visualize cross-validation results
plotcp(rpart_t) 

# detailed summary of splits
summary(rpart_t, pretty = T) 

# plot tree
plot(rpart_t, uniform=TRUE,
     main="Classification Tree")
text(rpart_t, use.n=TRUE, all=TRUE, cex=.5)

# Fitted Values in class probabilities
pred_1 = predict(rpart_t, df_t[,1:(ncol(df_t)-1)])

# Fitted response labels
res_1 = ifelse(pred_1[, 1] > 0.5, 0, 1)

# Plotting the data with fitted labels based on the model
#plot(sim.data1$x.1, sim.data1$x.2, pch = as.numeric(sim.data1$y),
#     col = factor(res))

# Misclassification errors in the training data
table(df_t$y, res_1)

```    
  
```{r, echo=F, message=F, warning=F, eval=F, include=F}

# too many NAs for tree() ???
tree_t = tree(y ~ ., tmp)

summary(tree_t)

plot(tree_t)
text(tree_t, pretty=0)
```
  
```{r, echo=F, message=F, warning=F, eval=F}  
## randomForests

set.seed(1)

# explicitly convert cat data to factors
#df_t_tmp = df_t
#df_t_tmp[, 1:2140] <- lapply(df_t_tmp[, 1:2140], as.factor)

bag_t = randomForest(y ~ ., data = tmp, mtry = 1998,
                          importance = TRUE, na.action = na.roughfix)
```  
  
```{r, echo=F, message=F, warning=F, eval=F}  
## randomForests

set.seed(1)

# explicitly convert cat data to factors
#df_t_tmp = df_t
#df_t_tmp[, 1:2140] <- lapply(df_t_tmp[, 1:2140], as.factor)

bag_t = randomForest(y ~ ., data = tmp, mtry = 1998,
                          importance = TRUE)
```    
  
### Random Forest with na.roughfix  
```{r, echo=F, message=F, warning=F}  
set.seed(1)

rf_t = randomForest(y ~ ., data = tmp, importance = TRUE, na.action=na.roughfix)

pred_rf_t = rf_t$predicted
table(tmp$y, pred_rf_t)
```
    
### Feature extraction  
  
```{r, echo=F, message=F, warning=F}
# try only numerical features?

# use only features from defective parts
#https://stackoverflow.com/questions/2643939/remove-columns-from-dataframe-where-all-values-are-na
not_all_na <- function(x) any(!is.na(x))

df_n_200k = read.csv("./bosch-production-line-performance/train_numeric.csv", header = T, nrows = 250000)

df_n_200k_ = df_n_200k %>%
  filter(Response==1) %>%
  select(
    where(not_all_na)
    )
n_nna_cols = colnames(df_n_200k_)[1:length(n_nna_cols)-1]
fail_ids = df_n_200k_[,"Id"]

rm(df_n_200k)
rm(df_n_200k_)
gc()

df_c_250k = read.csv("./bosch-production-line-performance/train_categorical.csv", header = T, nrows = 250000)
df_c_250k = df_c_250k[df_c_250k$Id %in% fail_ids,]
df_c_250k_ = df_c_250k %>%
  select(
    where(not_all_na)
    )
c_nna_cols = colnames(df_c_250k_)

rm(df_c_250k)
rm(df_c_250k_)
gc()

# separate features, response, ignore ID and X
df_nc_train_y_nna = df_nc_train[,ncol(df_nc_train)]
df_nc_train_X_nna = df_nc_train[,4:(ncol(df_nc_train)-1)]

keep_cols = union(c_nna_cols[2:length(c_nna_cols)], n_nna_cols[2:length(n_nna_cols)])

df_nna = data.frame(x=df_nc_train_X_nna[,keep_cols], y=factor(df_nc_train_y_nna))

# explicitly convert cat data to factors
df_nna_tmp = df_nna
df_nna_tmp[, 1:1450] <- lapply(df_nna_tmp[, 1:1450], as.factor)

# FIXME: this will vary from sample to sample
# remove cols of all NAs
tmp = df_nna_tmp[, colSums(is.na(df_nna_tmp)) < nrow(df_nna_tmp)]

# classification tree
rpart_nna = rpart(y~., data = tmp, method = 'class')

# display the results
printcp(rpart_nna) 

# visualize cross-validation results
plotcp(rpart_nna) 

# detailed summary of splits
summary(rpart_nna, pretty = T) 

# plot tree
plot(rpart_nna, uniform=TRUE,
     main="Classification Tree")
text(rpart_nna, use.n=TRUE, all=TRUE, cex=.5)

# Fitted Values in class probabilities
pred_nna = predict(rpart_nna, tmp[,1:(ncol(tmp)-1)])

# Fitted response labels
res_nna = ifelse(pred_nna[, 1] > 0.5, 0, 1)

# Misclassification errors in the training data
table(tmp$y, res_nna)

## PCA on numeric features only?
# classification tree w/ only numeric features
df_nna_n = data.frame(x=df_nc_train_X_nna[,n_nna_cols[2:length(n_nna_cols)]], y=factor(df_nc_train_y_nna))
df_nna_n = df_nna_n[, colSums(is.na(df_nna_n)) < nrow(df_nna_n)]
                                                     
rpart_nna_n = rpart(y~., data = df_nna_n, method = 'class')
summary(rpart_nna_n, pretty = T) 

pred_nna_n = predict(rpart_nna_n, df_nna_n)
res_nna_n = ifelse(pred_nna_n[, 1] > 0.5, 0, 1)
table(tmp$y, res_nna_n)

# boosting
gbm.sim1 = gbm.fit(x=df_nna_n[,1:942], y=(as.numeric(df_nna_n$y)-1),
                  distribution = 'adaboost', n.trees = 2500, nTrain = n*0.7,
                  verbose = F)
# Fitted response labels
pred.gbm.sim1 = ifelse(gbm.sim1$fit > 0, 1, 0)

# Misclassification errors in the training data
table(df_nna_n$y, pred.gbm.sim1)

# 
gbm.perf(gbm.sim1)


# xgboost
xgb.sim1 = xgboost(data = as.matrix(df_nna_n[,1:942]), label = (as.numeric(df_nna_n$y)-1), 
                  max.depth = 6, eta = 1, nrounds = 100,
                  objective = "binary:logistic", eval_metric = "logloss")

# Fitted response labels
pred.xgb.sim1 = predict(xgb.sim1, as.matrix(df_nna_n[,1:942]))
pred.xgb.sim1 = ifelse(pred.xgb.sim1 > 0.5, 1, 0)

# does well, likely overfitting
table(df_nna_n$y, pred.xgb.sim1)

```
  
  
### Single classification decision tree  
```{r, echo=F, message=F, warning=F}
# load train, validation set for single decision tree
df_nc_train = read.csv('train_40k.csv', header = T)
df_nc_val = read.csv('val_10k.csv', header = T)
last_train_row = dim(df_nc_train)[[1]]
df_nc_trainval= rbind(df_nc_train, df_nc_val)

# separate features, response, ignore ID and X
df_nc_tv_y = df_nc_trainval[,ncol(df_nc_trainval)]
df_nc_tv_X = df_nc_trainval[,4:(ncol(df_nc_trainval)-1)]

mean(df_nc_tv_y) # only 0.526% faulty

df_tv = data.frame(x=df_nc_tv_X, y=factor(df_nc_tv_y))

# explicitly convert cat data to factors
df_tv_tmp = df_tv
df_tv_tmp[, 1:2140] <- lapply(df_tv_tmp[, 1:2140], as.factor)

# FIXME: this will vary from sample to sample
# remove cols of all NAs
tmp_tv = df_tv_tmp[, colSums(is.na(df_tv_tmp)) < nrow(df_tv_tmp)]

# fit classification tree to training data
rpart_tree = rpart(y~., data = tmp_tv[1:last_train_row,], method = 'class')

# display the results
printcp(rpart_tree) 

# visualize cross-validation results
plotcp(rpart_tree) 

# detailed summary of splits
summary(rpart_tree, pretty = T) 

# plot tree
plot(rpart_tree, uniform=TRUE,
     main="Classification Tree")
text(rpart_tree, use.n=TRUE, all=TRUE, cex=.5)

# Fitted Values in class probabilities
pred_tree = predict(rpart_tree, tmp_tv[1:last_train_row,1:(ncol(tmp_tv)-1)])

# Fitted response labels
res_tree = ifelse(pred_tree[, 1] > 0.5, 0, 1)

# Plotting the data with fitted labels based on the model
#plot(sim.data1$x.1, sim.data1$x.2, pch = as.numeric(sim.data1$y),
#     col = factor(res))

# Misclassification errors in the training data
table(tmp_tv[1:last_train_row, ncol(tmp_tv)], res_tree)

# Fitted Values in class probabilities
pred_tree_v = predict(rpart_tree, tmp_tv[(last_train_row+1):nrow(tmp_tv),1:(ncol(tmp_tv)-1)])

# Fitted response labels
res_tree_v = ifelse(pred_tree_v[, 1] > 0.5, 0, 1)

# Misclassification errors in the training data
table(tmp_tv[(last_train_row+1):nrow(tmp_tv),ncol(tmp_tv)], res_tree_v)
```
  
### Random Forest    
  
  
### Boosting    
  
```{r, echo=F, message=F, warning=F}
# only numeric features, response
tmp_xg = df_tv_tmp[,2141:3109]

# drop all NA cols
tmp_xg = tmp_xg[, colSums(is.na(tmp_xg)) < nrow(tmp_xg)]

# boosting
gbm.sim1 = gbm.fit(x=as.matrix(tmp_xg[1:last_train_row,1:ncol(tmp_xg)-1]), 
                   y=(as.numeric(tmp_xg[1:last_train_row,ncol(tmp_xg)])-1),
                  distribution = 'adaboost', n.trees = 2500, nTrain = n*0.7,
                  verbose = F)
# Fitted response labels
pred.gbm.sim1 = ifelse(gbm.sim1$fit > 0, 1, 0)

# Misclassification errors in the training data
table(tmp_xg[1:last_train_row,ncol(tmp_xg)], pred.gbm.sim1)

# 
gbm.perf(gbm.sim1)

```
  
### Extreme Gradient Boosting      
  
```{r, echo=F, message=F, warning=F}
set.seed(1)

# xgboost
xgb.sim1 = xgboost(data = as.matrix(tmp_xg[1:last_train_row,1:ncol(tmp_xg)-1]), 
                   label = (as.numeric(tmp_xg[1:last_train_row,ncol(tmp_xg)])-1), 
                   max.depth = 6, eta = 1, nrounds = 100,
                   objective = "binary:logistic", eval_metric = "logloss")

# Fitted response labels
pred.xgb.sim1 = predict(xgb.sim1, 
                        as.matrix(tmp_xg[1:last_train_row,1:ncol(tmp_xg)-1]))
pred.xgb.sim1 = ifelse(pred.xgb.sim1 > 0.5, 1, 0)

# does well, likely overfitting
table(tmp_xg[1:last_train_row,ncol(tmp_xg)], pred.xgb.sim1)

# validation data
# Fitted Values in class probabilities
pred.xgb.v.sim1 = predict(xgb.sim1, as.matrix(tmp_xg[(last_train_row+1):nrow(tmp_xg),1:ncol(tmp_xg)-1]))
pred.xgb.v.sim1 = ifelse(pred.xgb.v.sim1 > 0.5, 1, 0)

# misclassification errors in the validation data
table(tmp_xg[(last_train_row+1):nrow(tmp_xg),ncol(tmp_xg)], pred.xgb.v.sim1)

## early exit
xgb.sim.cv = xgb.cv(data = as.matrix(tmp_xg[1:last_train_row,1:ncol(tmp_xg)-1]), 
                    label = (as.numeric(tmp_xg[1:last_train_row,ncol(tmp_xg)])-1), 
                    max.depth = 6, eta = 1, nrounds = 100,
                    nfold = 5, early_stopping_rounds = 5,
                    objective = "binary:logistic", eval_metric = "logloss")
sel_rounds = xgb.sim.cv$best_iteration
xgb.sim2 = xgboost(data = as.matrix(tmp_xg[1:last_train_row,1:ncol(tmp_xg)-1]), 
                   label = (as.numeric(tmp_xg[1:last_train_row,ncol(tmp_xg)])-1), 
                   max.depth = 6, eta = 1, nrounds = sel_rounds,
                   objective = "binary:logistic", eval_metric = "logloss")

# training data
pred.xgb.sim2= predict(xgb.sim2, as.matrix(tmp_xg[1:last_train_row,1:ncol(tmp_xg)-1]))
pred.xgb.sim2 = ifelse(pred.xgb.sim2 > 0.5, 1, 0)

# 
table(tmp_xg[1:last_train_row,ncol(tmp_xg)], pred.xgb.sim2)

# validation data
pred.xgb.v.sim2 = predict(xgb.sim2, as.matrix(tmp_xg[(last_train_row+1):nrow(tmp_xg),1:ncol(tmp_xg)-1]))
pred.xgb.v.sim2 = ifelse(pred.xgb.v.sim2 > 0.5, 1, 0)
table(tmp_xg[(last_train_row+1):nrow(tmp_xg),ncol(tmp_xg)], pred.xgb.v.sim2)
```

### Appendix: R code
```{r ref.label=knitr::all_labels(), echo = T, eval = F}
```
